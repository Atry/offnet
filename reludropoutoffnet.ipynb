{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iteration_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import debug as tf_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "training_data, testing_data = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = np.max(testing_data[1]).__int__() + 1\n",
    "number_of_input_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_hidden_layers = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_computing_layers = number_of_hidden_layers + number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_layers = number_of_input_channels + number_of_computing_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_number_samples_per_layer = number_of_layers # All the previous layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddings = (0, 0), (2, 2), (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = number_of_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale = 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6160\n"
     ]
    }
   ],
   "source": [
    "dense_weight_size = (number_of_layers + number_of_input_channels) * (number_of_layers - number_of_input_channels) // 2\n",
    "print(dense_weight_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_training_layers = 50\n",
    "# dropout_rate = (number_of_hidden_layers - number_of_training_layers) / number_of_hidden_layers\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_sample(images, offset_x, offset_y):\n",
    "    tf.assert_rank(images, 3)\n",
    "    image_shape = tf.shape(images)\n",
    "    current_batch_size = image_shape[0]\n",
    "    width = image_shape[1]\n",
    "    height = image_shape[2]\n",
    "    \n",
    "    def check_width(tensor):\n",
    "        if (tf.test.is_gpu_available()):\n",
    "            return tensor\n",
    "        else:\n",
    "            return tf.maximum(tf.minimum(tensor, width - 1), 0)\n",
    "    \n",
    "    def check_height(tensor):\n",
    "        if (tf.test.is_gpu_available()):\n",
    "            return tensor\n",
    "        else:\n",
    "            return tf.maximum(tf.minimum(tensor, height - 1), 0)\n",
    "    \n",
    "    offset_left = tf.floor(offset_x)\n",
    "    offset_top = tf.floor(offset_y)\n",
    "    tf.assert_equal(tf.shape(offset_left), image_shape)\n",
    "    tf.assert_equal(tf.shape(offset_top), image_shape)\n",
    "\n",
    "    \n",
    "    eps = 1e-7\n",
    "\n",
    "    factor_right = offset_x - offset_left + eps\n",
    "    factor_left = 1.0 + 2.0 * eps - factor_right\n",
    "    factor_bottom = offset_y - offset_top + eps\n",
    "    factor_top = 1.0 + 2.0 * eps - factor_bottom\n",
    "    \n",
    "    image_index, x_index, y_index = tf.meshgrid(\n",
    "        tf.range(current_batch_size, dtype=tf.int32),\n",
    "        tf.range(width, dtype=tf.int32),\n",
    "        tf.range(height, dtype=tf.int32),\n",
    "        indexing='ij',\n",
    "    )\n",
    "    \n",
    "    tf.assert_equal(tf.shape(image_index), image_shape)\n",
    "    tf.assert_equal(tf.shape(x_index), image_shape)\n",
    "    tf.assert_equal(tf.shape(y_index), image_shape)\n",
    "\n",
    "    \n",
    "    left_index = check_width(x_index + tf.cast(offset_left, tf.int32))\n",
    "    top_index = check_height(y_index + tf.cast(offset_top, tf.int32))\n",
    "\n",
    "    right_index = left_index + 1\n",
    "    bottom_index = top_index + 1\n",
    "    \n",
    "    tf.assert_equal(tf.shape(left_index), image_shape)\n",
    "    tf.assert_equal(tf.shape(top_index), image_shape)\n",
    "    tf.assert_equal(tf.shape(right_index), image_shape)\n",
    "    tf.assert_equal(tf.shape(bottom_index), image_shape)    \n",
    "    \n",
    "    tf.assert_rank(left_index, 3)\n",
    "    tf.assert_rank(right_index, 3)\n",
    "    tf.assert_rank(bottom_index, 3)\n",
    "    tf.assert_rank(top_index, 3)\n",
    "    \n",
    "    images_top_left = tf.gather_nd(images, tf.stack((image_index, left_index, top_index), axis=3))\n",
    "    images_top_right = tf.gather_nd(images, tf.stack((image_index, right_index, top_index), axis=3))\n",
    "    images_bottom_left = tf.gather_nd(images, tf.stack((image_index, left_index, bottom_index), axis=3))\n",
    "    images_bottom_right = tf.gather_nd(images, tf.stack((image_index, right_index, bottom_index), axis=3))\n",
    "\n",
    "    tf.assert_rank(images_top_left, 3)\n",
    "    tf.assert_rank(images_top_right, 3)\n",
    "    tf.assert_rank(images_bottom_left, 3)\n",
    "    tf.assert_rank(images_bottom_right, 3)\n",
    "\n",
    "    lerp_top = factor_left * images_top_left + factor_right * images_top_right\n",
    "    lerp_bottom = factor_left * images_bottom_left + factor_right * images_bottom_right\n",
    "    output = factor_top * lerp_top + factor_bottom * lerp_bottom\n",
    "    tf.assert_rank(output, 3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_wise_dropout():\n",
    "    enabled_hidden_layer_ids = number_of_input_channels + tf.squeeze(\n",
    "        tf.to_int32(tf.where(\n",
    "            tf.random_uniform((number_of_hidden_layers, )) >= dropout_rate\n",
    "        )),\n",
    "        axis=1\n",
    "    )\n",
    "    return tf.concat(\n",
    "        (\n",
    "            tf.range(number_of_input_channels),\n",
    "            enabled_hidden_layer_ids,\n",
    "            tf.range(number_of_input_channels + number_of_hidden_layers, number_of_layers)\n",
    "        ),\n",
    "        axis=0\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params, config):\n",
    "    random.seed(number_of_hidden_layers)\n",
    "    padded_input = tf.pad(tf.cast(features, tf.float32) / 255.0 - 0.5, paddings)\n",
    "    \n",
    "    enabled_layer_ids = layer_wise_dropout() if mode == tf.estimator.ModeKeys.TRAIN else tf.range(number_of_classes)\n",
    "    number_of_enabled_layers = tf.shape(enabled_layer_ids)[0]\n",
    "    \n",
    "    score_weight = tf.get_variable(\n",
    "        name=\"score_weight\",\n",
    "        initializer=tf.random_normal_initializer(),\n",
    "        dtype=tf.float32,\n",
    "        shape=(dense_weight_size, )\n",
    "    )\n",
    "    offset_x_weight = tf.get_variable(\n",
    "        name=\"offset_x_weight\",\n",
    "        initializer=tf.random_normal_initializer(),\n",
    "        dtype=tf.float32,\n",
    "        shape=(dense_weight_size, )\n",
    "    )\n",
    "    offset_y_weight = tf.get_variable(\n",
    "        name=\"offset_y_weight\",\n",
    "        initializer=tf.random_normal_initializer(),\n",
    "        dtype=tf.float32,\n",
    "        shape=(dense_weight_size, )\n",
    "    )\n",
    "    score_bias = tf.get_variable(\n",
    "        name=\"score_bias\",\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        dtype=tf.float32,\n",
    "        shape=(number_of_layers, ),\n",
    "    )\n",
    "    offset_x_bias = tf.get_variable(\n",
    "        name=\"offset_x_bias\",\n",
    "        initializer=tf.random_uniform_initializer(minval=-3.0, maxval=3.0),\n",
    "        dtype=tf.float32,\n",
    "        shape=(number_of_layers, ),\n",
    "    )\n",
    "    offset_y_bias = tf.get_variable(\n",
    "        name=\"offset_y_bias\",\n",
    "        initializer=tf.random_uniform_initializer(minval=-3.0, maxval=3.0),\n",
    "        dtype=tf.float32,\n",
    "        shape=(number_of_layers, ),\n",
    "    )\n",
    "\n",
    "    tf.assert_rank(enabled_layer_ids, 1)\n",
    "    \n",
    "    def score_map(previous_layers, i):\n",
    "        layer_id = enabled_layer_ids[i]\n",
    "        enabled_previous_layer_ids = enabled_layer_ids[:i]\n",
    "\n",
    "        weight_start = (number_of_input_channels + layer_id) * (layer_id - number_of_input_channels) // 2\n",
    "        tf.assert_rank(weight_start, 0)\n",
    "        weight_indicies = enabled_previous_layer_ids + weight_start\n",
    "        \n",
    "        # TODO test if the performance improves when using SparseTensor\n",
    "        previous_layer_tensor = previous_layers.gather(tf.range(i))\n",
    "        tf.assert_rank(previous_layer_tensor, 4)\n",
    "\n",
    "        def indexed_sum(weight, bias):\n",
    "            tf.assert_rank(weight, 1)\n",
    "            gather_weight = tf.gather(\n",
    "                params=weight,\n",
    "                indices=weight_indicies\n",
    "            ) * tf.sqrt(weight_scale / tf.to_float(i))\n",
    "\n",
    "            tf.assert_rank(gather_weight, 1)\n",
    "            tf.assert_equal(tf.shape(gather_weight)[0], tf.shape(previous_layer_tensor)[0])\n",
    "            return tf.tensordot(\n",
    "                gather_weight,\n",
    "                previous_layer_tensor,\n",
    "                axes=1\n",
    "            ) + bias[layer_id]\n",
    "\n",
    "        layer = grid_sample(\n",
    "            indexed_sum(score_weight, score_bias),\n",
    "            indexed_sum(offset_x_weight, offset_x_bias),\n",
    "            indexed_sum(offset_y_weight, offset_y_bias),\n",
    "        )\n",
    "        return layer\n",
    "\n",
    "    layers, i = tf.while_loop(\n",
    "        lambda layers, i: i < number_of_enabled_layers,\n",
    "        # TODO: layers for output classes should be independent to each other\n",
    "        lambda layers, i: (layers.write(i, score_map(layers, i)), i + 1),\n",
    "        tf.while_loop(\n",
    "            lambda layers, i: i < number_of_enabled_layers - number_of_classes,\n",
    "            lambda layers, i: (layers.write(i, tf.nn.relu(score_map(layers, i))), i + 1),\n",
    "            (\n",
    "                tf.TensorArray(tf.float32, size=number_of_layers, clear_after_read=False, infer_shape=True).write(\n",
    "                    0,\n",
    "                    padded_input,\n",
    "                ),\n",
    "                tf.constant(number_of_input_channels),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tf.assert_equal(i, tf.cast(number_of_layers, tf.int32))\n",
    "\n",
    "    scores_per_pixel = layers.gather(tf.range(number_of_enabled_layers - number_of_classes, number_of_enabled_layers, dtype=tf.int32))\n",
    "    score_shape = tf.shape(scores_per_pixel)\n",
    "    scores = tf.transpose(scores_per_pixel[:, :, score_shape[2] // 2, score_shape[3] // 2])\n",
    "    probabilities = tf.nn.softmax(logits=scores)\n",
    "    predicted_classes = tf.argmax(scores, 1)\n",
    "    predictions = {\n",
    "        'probabilities' : probabilities,\n",
    "        'scores': scores,\n",
    "        'class': predicted_classes,\n",
    "    }\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    if labels is None:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "        )\n",
    "    else:\n",
    "        loss = tf.losses.softmax_cross_entropy(logits=scores, onehot_labels=tf.one_hot(labels, number_of_classes))\n",
    "        eval_metric_ops = {\n",
    "            'accuracy': tf.metrics.accuracy(labels=labels, predictions=predicted_classes)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            train_op=optimizer.minimize(loss, global_step=tf.train.get_global_step()),\n",
    "            eval_metric_ops=eval_metric_ops,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=f\"models/reludropoutoffnet{number_of_hidden_layers}\",\n",
    "    session_config=tf.ConfigProto(\n",
    "        gpu_options=tf.GPUOptions(\n",
    "            allow_growth=True,\n",
    "        ),\n",
    "        graph_options=tf.GraphOptions(\n",
    "            optimizer_options=tf.OptimizerOptions(\n",
    "                global_jit_level=tf.OptimizerOptions.ON_2,\n",
    "                do_function_inlining=True,\n",
    "                do_constant_folding=True,\n",
    "                do_common_subexpression_elimination=True,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'models/reludropoutoffnet100', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "    do_common_subexpression_elimination: true\n",
      "    do_constant_folding: true\n",
      "    do_function_inlining: true\n",
      "    global_jit_level: ON_2\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3fb76c64e0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f4044125950>) includes params argument, but params are not passed to Estimator.\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn, config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def training_dataset():\n",
    "    return tf.data.Dataset.from_tensor_slices(training_data).shuffle(1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_dataset():\n",
    "    return tf.data.Dataset.from_tensor_slices(testing_data).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook = tf_debug.TensorBoardDebugHook(\"localhost:6064\")\n",
    "# estimator.train(training_dataset,hooks=[hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into models/reludropoutoffnet100/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3091843, step = 0\n",
      "INFO:tensorflow:global_step/sec: 2.55887\n",
      "INFO:tensorflow:loss = 2.1861434, step = 100 (39.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.62768\n",
      "INFO:tensorflow:loss = 2.1515114, step = 200 (38.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.63595\n",
      "INFO:tensorflow:loss = 1.9226022, step = 300 (37.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.63728\n",
      "INFO:tensorflow:loss = 1.9615873, step = 400 (37.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.63025\n",
      "INFO:tensorflow:loss = 2.0977364, step = 500 (38.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.59078\n",
      "INFO:tensorflow:loss = 1.7248238, step = 600 (38.598 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.62409\n",
      "INFO:tensorflow:loss = 1.8176091, step = 700 (38.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.65051\n",
      "INFO:tensorflow:loss = 1.3605891, step = 800 (37.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.63061\n",
      "INFO:tensorflow:loss = 1.5793331, step = 900 (38.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.65354\n",
      "INFO:tensorflow:loss = 1.8923829, step = 1000 (37.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.6532\n",
      "INFO:tensorflow:loss = 1.2836889, step = 1100 (37.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.63259\n",
      "INFO:tensorflow:loss = 1.2711072, step = 1200 (37.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.64431\n",
      "INFO:tensorflow:loss = 1.1301012, step = 1300 (37.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.64755\n",
      "INFO:tensorflow:loss = 1.2652587, step = 1400 (37.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.64525\n",
      "INFO:tensorflow:loss = 1.0603044, step = 1500 (37.804 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1547 into models/reludropoutoffnet100/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.0796242.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-11-06:01:45\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/reludropoutoffnet100/model.ckpt-1547\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-11-06:01:48\n",
      "INFO:tensorflow:Saving dict for global step 1547: accuracy = 0.145625, global_step = 1547, loss = 4.7933607\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/reludropoutoffnet100/model.ckpt-1547\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1548 into models/reludropoutoffnet100/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.8009906, step = 1547\n",
      "INFO:tensorflow:global_step/sec: 2.59425\n",
      "INFO:tensorflow:loss = 1.410197, step = 1647 (38.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.63857\n",
      "INFO:tensorflow:loss = 1.137208, step = 1747 (37.897 sec)\n"
     ]
    }
   ],
   "source": [
    "tf.estimator.train_and_evaluate(\n",
    "    estimator,\n",
    "    train_spec=tf.estimator.TrainSpec(training_dataset),\n",
    "    eval_spec=tf.estimator.EvalSpec(testing_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.evaluate(input_fn=lambda:tf.data.Dataset.from_tensor_slices(testing_data).batch(3).take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple(estimator.predict(input_fn=lambda:tf.data.Dataset.from_tensor_slices(testing_data).batch(3).take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.train(input_fn=training_dataset, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
